{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from smart_open import smart_open\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_titles = ['Principles_of_morals_Hume', 'Political_discourses_Hume', 'Dialogues_natural_religion_Hume','Concerning_human_understanding_Hume',\n",
    "'Cratylus_Plato', 'Apology_criton_phaedo_Plato', 'Gorgias_Plato', 'Republic_Plato',\n",
    " 'History_peter_great_Voltaire', 'Socrates_Voltaire', 'Philosophical_dictionary_Voltaire', 'Candide_Voltaire',\n",
    " 'Analysis_Mind_Russell', 'Mysticism_logic_Russell', 'Problems_philosophy_Russell', 'Roads_freedom_Russell', \n",
    " 'Pure_reason_Kant', 'Practical_reason_Kant', 'Judgment_Kant', 'Perpetual_peace_Kant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(file_path = str):\n",
    "    text = open(f'data_txt/{file_path}.txt').read().lower() # reads the file and convert all string to lowercase.\n",
    "    replace = ['_', '°', ' ', '*', '\\n', '—', '|', '\\t', '\\u200b', '§', 'ç', '^'] # list of symbols to be replace.\n",
    "    text_replace = re.sub(f'{replace}', ' ', text) # with regex it substitutes the symbols if the list replace in the text with a space.\n",
    "    text_clean = re.sub(r'(\\[.+?\\])|(\\{.+?\\})', '', text_replace) # substitutes parenthesis and curly brackets.\n",
    "    print('Text cleaned, now making tokens.')\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm') # loads spacy module for processing english words. \n",
    "    nlp.max_length = 10000000 # augments the length of characters the nlp object can receive. \n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS # loads the stopwords from spacy.\n",
    "    print('Spacy completely load.')\n",
    "    doc = nlp(text_clean) # converts the text into a spacy doc with many linguistic features\n",
    "    text_tokenized = [token.lemma_ for token in doc if not token.is_stop] # iterates over each word in doc and checks if its not a stop word \n",
    "    print('Text converted into token lemmas.\\n')                          # and return de word's lexeme.\n",
    "    return text_tokenized\n",
    "\n",
    "    \n",
    "def save_txt(document):\n",
    "    np.savetxt(f'data_clean/document.txt', # from numpy calls a function to save txt files.\n",
    "               np.array(document), \n",
    "               newline='\\n', \n",
    "               encoding='utf-8', \n",
    "               fmt=\"%s\")\n",
    "    print('Text saved in folder.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_texts = [preprocessing(title) for title in text_titles[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document = [word for text in clean_texts for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_txt(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(file_name):\n",
    "    '''\n",
    "    This function creates a dictionary, bag of words and TF-IDF matrix\n",
    "    from a text file.\n",
    "    It also saves dictionary, BOW and TF-IDF objects to disk.\n",
    "    Brisa's Function\n",
    "    '''\n",
    "    #Create gensim dictionary\n",
    "    dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open(f'data_clean/{file_name}.txt', encoding='utf-8'))\n",
    "    print(\"Created Dictionary.\\nFound {} words.\\n\".format(len(dictionary.values())))\n",
    "    \n",
    "    #Filter dictionary for common words\n",
    "    #dictionary.filter_extremes(no_above=0.5, no_below=300)\n",
    "    #dictionary.compactify()\n",
    "    #print(\"Filtered Dictionary.\\nLeft with {} words.\\n\".format(len(dictionary.values())))\n",
    "    \n",
    "    #Create Bag of Words\n",
    "    bow = []\n",
    "    for line in smart_open(f'data_clean/{file_name}.txt', encoding='utf-8'):\n",
    "        tokenized_list = simple_preprocess(line, deacc=True)\n",
    "        bow.append(dictionary.doc2bow(tokenized_list, allow_update=True))\n",
    "    print(\"Created Bag of Words.\\n\".format(len(bow)))\n",
    "    \n",
    "    #Create TF-IDF Matrix\n",
    "    tfidf = TfidfModel(bow, smartirs='ntc')\n",
    "    tfidf_corpus = tfidf[bow]\n",
    "    print(\"Created TF-IDF matrix.\\n\".format(len(tfidf_corpus)))\n",
    "     \n",
    "    #Save files to disk\n",
    "    dictionary.save(f'dictionary_corpus/{file_name}_dictionary.dict')\n",
    "    print('Saved dictionary object to disk.')\n",
    "    corpora.MmCorpus.serialize(f'dictionary_corpus/{file_name}_bow_corpus.mm', bow)\n",
    "    print('Saved bag of words corpus object to disk.')\n",
    "    corpora.MmCorpus.serialize(f'dictionary_corpus/{file_name}_tfidf_corpus.mm', tfidf_corpus)\n",
    "    print('Saved TF-IDF corpus object to disk.')\n",
    "    \n",
    "    return print('Processed Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n",
      "Text converted into token lemmas.\n",
      "\n",
      "Text cleaned, now making tokens.\n",
      "Spacy completely load.\n"
     ]
    }
   ],
   "source": [
    "clean_texts = [preprocessing(title) for title in text_titles]\n",
    "document = [word for text in clean_texts for word in text]\n",
    "save_txt(document)\n",
    "prepare_corpus('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
